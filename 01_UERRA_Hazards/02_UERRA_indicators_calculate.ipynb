{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hazard assessment: Compute climate indicators\n",
    "\n",
    "- A workflow from the CLIMAAX [Handbook](https://handbook.climaax.eu/) and [MULTI_infrastructure](https://github.com/CLIMAAX/MULTI_infrastructure) GitHub repository.\n",
    "- See our [how to use risk workflows](https://handbook.climaax.eu/notebooks/workflows_how_to.html) page for information on how to run this notebook.\n",
    "\n",
    "The impact of the single hazard (e.g. extreme precipitation, heatwaves) can be evaluated on the different assets of the infrastructure, for example on the terminal, runway, taxiways, parking areas etc.\n",
    "\n",
    "The data used in this section can be found on Copernicus Climate Data Store (CDS) and they are related to [UERRA MESCAN-SURFEX](https://cds.climate.copernicus.eu/datasets/reanalysis-uerra-europe-single-levels?tab=download)\n",
    "on single level.\n",
    "The automatic download was made through Copernicus API (`cdsapi` Python package) both for temperature and precipitation data in the [previous step](01_UERRA_preprocessing.ipynb).\n",
    "\n",
    "The UERRA dataset was chosen for it's horizontal and temporal resolution, providing a quite high resolution with continuous measurement.\n",
    "However, the 5.5 km horizontal resolution could be too coarse to evaluate this type of indicator for individual parts of an airport (e.g., taxiways and runway)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparation work\n",
    "\n",
    "### Load libraries\n",
    "\n",
    "[xclim](https://xclim.readthedocs.io/en/stable/index.html) is a Python library tailored for climate services, offering a wide range of tools to compute climate-related indicators.\n",
    "In this notebook, it is the primary library used to calculate the climate indicator \"number of days above a threshold temperature\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "from scipy.stats import gumbel_r\n",
    "from tqdm import tqdm\n",
    "import xarray as xr\n",
    "import xclim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Area of interest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "region_name = 'IT'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Path configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Folder with temperature and total precipitation data\n",
    "folder = f'../data_{region_name}'\n",
    "\n",
    "# Output folder for the climate indicators\n",
    "output_folder = os.path.join(folder, 'indicators/uerra')\n",
    "os.makedirs(output_folder, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process the data\n",
    "\n",
    "Process the temperature and precipitation files from UERRA within `folder` and calculate climate indicators.\n",
    "Specifically:\n",
    "\n",
    "- Calculate the **number of days exceeding specific temperature thresholds** (35, 40 and 45Â° Celcius).\n",
    "- Calculate the **percentiles for both temperature and precipitation**.\n",
    "- Estimate **return levels for extreme precipitation events**.\n",
    "\n",
    "The results are saved as new NetCDF files in the path configured in `output_folder`.\n",
    "We avoid recomputation by checking if output files already exist and handle missing values to avoid errors in the calculations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Temperature\n",
    "\n",
    "#### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = xr.open_mfdataset(os.path.join(folder, '*-t2m.nc'))\n",
    "\n",
    "# Compute maximum daily temperature and convert to Celsius\n",
    "dailyMaxTemp = ds['t2m'].resample(time='D').max().chunk({'time': -1})\n",
    "dailyMaxTemp.attrs['units'] = 'C'  # Set the units"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Number of days above thresholds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the output files for Number of Days Above Thresholds\n",
    "output_files_numDays = {\n",
    "    'NumbDaysAbove30.nc': '30 C',\n",
    "    'NumbDaysAbove35.nc': '35 C',\n",
    "    'NumbDaysAbove40.nc': '40 C',\n",
    "    'NumbDaysAbove45.nc': '45 C',\n",
    "}\n",
    "\n",
    "for fname, threshold in output_files_numDays.items():\n",
    "    output_path = os.path.join(output_folder, fname)\n",
    "\n",
    "    if os.path.exists(output_path):\n",
    "        print(f'The file {fname} already exists. Skipping calculation.')\n",
    "    else:\n",
    "        # Compute the number of days above the threshold\n",
    "        with xclim.set_options(cf_compliance='log'):\n",
    "            NumbDaysAbove = xclim.atmos.tx_days_above(tasmax=dailyMaxTemp, thresh=threshold, freq='YS')\n",
    "            NumbDaysAbove_avg = NumbDaysAbove.mean(dim='time', skipna=True)\n",
    "\n",
    "            # Save it\n",
    "            NumbDaysAbove_avg.to_netcdf(output_path)\n",
    "            print(f'Saved {fname} to {output_path}.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Percentiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the output files for Percentiles\n",
    "output_files_percent = {\n",
    "    'Temp_P95.nc': 0.95,\n",
    "    'Temp_P999.nc': 0.999,\n",
    "}\n",
    "\n",
    "for fname_percent, percentile in output_files_percent.items():\n",
    "    output_path_percent = os.path.join(output_folder, fname_percent)\n",
    "\n",
    "    if os.path.exists(output_path_percent):\n",
    "        print(f\"The file {fname_percent} already exists. Skipping calculation.\")\n",
    "    else:\n",
    "        # Calculate the percentiles across all time steps\n",
    "        dailyMaxTemp_nonan = dailyMaxTemp.dropna(dim='time', how='all')\n",
    "        calc_percentile = dailyMaxTemp_nonan.quantile(percentile, dim='time')\n",
    "\n",
    "        # Save it\n",
    "        calc_percentile.to_netcdf(output_path_percent)\n",
    "        print(f\"Saved {percentile * 100}th percentile to {output_path_percent}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Precipitation\n",
    "\n",
    "#### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = xr.open_mfdataset(os.path.join(folder, \"*-tp.nc\"))\n",
    "\n",
    "dailyTotPrep = ds['tp'].chunk({'time': -1})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Percentiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the output files for Percentiles\n",
    "output_files_percent_prec = {\n",
    "    'Precip_P99.nc': 0.99,\n",
    "    'Precip_P995.nc': 0.995,\n",
    "    'Precip_P999.nc': 0.999}\n",
    "\n",
    "for fname_percent_prep, percentile_prep in output_files_percent_prec.items():\n",
    "    output_path_percent_prep = os.path.join(output_folder, fname_percent_prep)\n",
    "\n",
    "    if os.path.exists(output_path_percent_prep):\n",
    "        print(f\"The file {fname_percent_prep} already exists. Skipping calculation.\")\n",
    "    else:\n",
    "        # Calculate the percentiles across all time steps\n",
    "        dailyTotPrep_nonan = dailyTotPrep.dropna(dim='time', how='all')\n",
    "        calc_percentile_prep = dailyTotPrep_nonan.quantile(percentile_prep, dim='time')\n",
    "\n",
    "        # Save it\n",
    "        calc_percentile_prep.to_netcdf(output_path_percent_prep)\n",
    "        print(f\"Saved {percentile_prep * 100}th percentile to {output_path_percent_prep}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Return levels for extreme precipitation events"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate the maximum one-day precipitation per year:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "annual_max_precip = dailyTotPrep.resample(time='YE').max().compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define Return Periods and Exceedance Probabilities: Return periods (e.g., 10, 20, 30, 50, 100, 150 years) are specified, and their corresponding exceedance probabilities are calculated. The exceedance probability represents the likelihood of surpassing a certain precipitation threshold in any given year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "return_periods = np.array([10, 20, 30, 50, 100, 150])\n",
    "exceedance_probs = 1 - (1 / return_periods)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loop over every grid cell (longitude, and latitude) and extract the annual maximum precipitation values for each.\n",
    "For each grid cell the Gumbel distribution (`gumbel_r.fit()`) is fitted to the annual maxima series to determine location (`loc`) and scale (`scale`) parameters.\n",
    "Then we calculate the return levels for the specified return periods using the previous parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare a new dataset to store return levels for each period and each grid cell\n",
    "return_levels_ds = xr.Dataset()\n",
    "\n",
    "# Loop over each grid cell (x, y) and fit the Weibull distribution\n",
    "for x in tqdm(annual_max_precip['x']):\n",
    "    for y in annual_max_precip['y']:\n",
    "        # Extract the annual maximum series for the current grid cell\n",
    "        annual_max_values = annual_max_precip.sel(x=x, y=y).values\n",
    "        annual_max_values = annual_max_values[~np.isnan(annual_max_values)]  # Remove NaNs\n",
    "\n",
    "        if len(annual_max_values) > 0:\n",
    "            # Fit the Gumbel distribution to the annual maxima for this grid cell\n",
    "            loc, scale = gumbel_r.fit(annual_max_values)\n",
    "\n",
    "            # Calculate the exceedance probabilities for the return periods\n",
    "            exceedance_probs = 1 - (1 / np.asarray(return_periods))\n",
    "\n",
    "            # Calculate return levels for the specified return periods using the Gumbel parameters\n",
    "            return_levels = gumbel_r.ppf(exceedance_probs, loc, scale)\n",
    "\n",
    "            # Store return levels in the dataset for each return period\n",
    "            for rp, rl in zip(return_periods, return_levels):\n",
    "                return_period_label = f\"return_period_{rp}_y\"\n",
    "                if return_period_label not in return_levels_ds:\n",
    "                    return_levels_ds[return_period_label] = xr.full_like(annual_max_precip.isel(time=0), np.nan)\n",
    "                return_levels_ds[return_period_label].loc[{'x': x, 'y': y}] = rl\n",
    "\n",
    "# Add coordinates and attributes to the new dataset\n",
    "return_levels_ds['x'] = annual_max_precip['x']\n",
    "return_levels_ds['y'] = annual_max_precip['y']\n",
    "return_levels_ds.attrs['description'] = 'Return levels for specified return periods based on GEV distribution fit'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `return_levels_ds` dataset represents the precipitation amount expected for each return period in each grid cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the return levels to a NetCDF file\n",
    "return_levels_ds.to_netcdf('../indicators/uerra/return_levels_gumbel.nc')\n",
    "return_levels_ds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next step\n",
    "\n",
    "[Visualize the computed climate indicators](./03_UERRA_indicators_plots.ipynb)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
