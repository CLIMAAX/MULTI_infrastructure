{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "84851655-1184-4c17-a1b9-df42bdc98ca8",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Hazard assessment for Infrastructures using Euro-Cordex datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b67a962",
   "metadata": {},
   "source": [
    "## Calculation of the indicator \"Percentiles of the temperature\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b378376d-aaf3-44b9-b331-bca0c125a749",
   "metadata": {
    "tags": []
   },
   "source": [
    "- See our [how to use risk workflows](https://handbook.climaax.eu/notebooks/workflows_how_to.html) page for information on how to run this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dd026bd",
   "metadata": {},
   "source": [
    "## Hazard assessment methodology\n",
    "We utilized outputs from 14 models within the EURO-CORDEX framework to evaluate hazards affecting infrastructure, in this notebook we used the total daily precipitation as an indicator of the hazzard. Our analysis included three Representative Concentration Pathways (RCPs): RCP2.6, RCP4.5, and RCP8.5. To structure the future projections, we divided the RCP scenarios into three distinct periods: 2021–2050, 2041–2070, and 2071–2100. Additionally, we used the historical period (1981–2010) as a baseline for comparison.\n",
    "\n",
    "For each period, we calculated the ensemble average across all 14 models, producing a single representative dataset for each timeframe, including both the historical and RCP scenarios. Using these averaged datasets, we computed the percentile of the daily maximum temperature 95 and 99.9 for each time period.\n",
    "\n",
    "Finally, we calculated the anomalies by subtracting the historical dataset values from each of the future scenario datasets to quantify changes. The results of these computations will be visualized in the notebook *05_cordexTempPercentiles_plots.ipynb* to assess potential future hazards."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c7f97fe-2578-4ce9-b4c7-1fe40214f045",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Preparation work"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd12ce07",
   "metadata": {},
   "source": [
    "### Select area of interest\n",
    "Before downloading the data, we will define the coordinates of the area of interest, for this workflow we selected the Italy region. Based on the shapefile of the country we will be able to clip the datasets for further processing, and display hazard and damage maps for this area."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6453335",
   "metadata": {},
   "source": [
    "### Load libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c445681b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import xarray as xr\n",
    "import xclim\n",
    "import re\n",
    "\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "438e0df7",
   "metadata": {},
   "source": [
    "### Create the directory structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79e25ef6-9a3b-4d42-a2b5-d787c2b70878",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define paths\n",
    "nc_files = \"/work/cmcc/dg07124/climax/data/cordex\"\n",
    "general_path = \"/work/cmcc/dg07124/climax/indicators/cordex\"\n",
    "subfolders = ['historical','rcp26', 'rcp45', 'rcp85']\n",
    "\n",
    "# Temperature thresholds\n",
    "percentiles = ['0.95', '0.999']\n",
    "\n",
    "# Time ranges to process\n",
    "rcp_time_ranges = [('2021', '2050'), ('2041', '2070'), ('2071', '2100')]\n",
    "historical_time_range = [('1981', '2010')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "262714f0-48e3-451c-a1c3-186a056f3286",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to process each NetCDF file for a given time range\n",
    "def process_file(file_path, percentile, save_path, start_year, end_year):\n",
    "    print(\"---------------------------------------------------\")\n",
    "    print(f\"Processing {file_path} for time range {start_year}-{end_year} and {percentile}\")\n",
    "    ds = xr.open_dataset(file_path)\n",
    "\n",
    "    # Select daily max temperature for the given time range\n",
    "    ds_sliced = ds.sel(time=slice(start_year, end_year))\n",
    "    dailyMaxTemp = (ds_sliced['tasmax'] - 273.15).resample(time='D').max()\n",
    "    dailyMaxTemp.attrs['units'] = 'C'\n",
    "    \n",
    "    # Get the minimum and maximum values\n",
    "    min_value = dailyMaxTemp.min(skipna=True).item()  # Convert to a scalar with .item()\n",
    "    max_value = dailyMaxTemp.max(skipna=True).item()\n",
    "\n",
    "    # Print the results\n",
    "    print(f\"Temp min value: {min_value}\")\n",
    "    print(f\"Temp max value: {max_value}\")\n",
    "\n",
    "    # Calculate the number of days above the threshold using xclim\n",
    "\n",
    "    # Calculate the percentiles across all time steps\n",
    "    dailyMaxTemp_nonan = dailyMaxTemp.dropna(dim='time', how='all')\n",
    "    calc_percentile = dailyMaxTemp_nonan.quantile(percentile, dim='time')\n",
    "\n",
    "\n",
    "    # Create the new filename with the time range and threshold information\n",
    "    filename = os.path.basename(file_path)  # Extract original filename\n",
    "    file_name_no_ext = os.path.splitext(filename)[0]  # Remove extension\n",
    "    new_filename = f\"{file_name_no_ext}_p{percentile}_{start_year}-{end_year}.nc\"\n",
    "\n",
    "    # Save the result to the new file path\n",
    "    calc_percentile.to_netcdf(os.path.join(save_path, new_filename))\n",
    "\n",
    "    # Get the minimum and maximum values\n",
    "    min_value_indic = calc_percentile.min(skipna=True).item()  # Convert to a scalar with .item()\n",
    "    max_value_indic = calc_percentile.max(skipna=True).item()\n",
    "\n",
    "    # Print the results\n",
    "    print(f\"Minimum percentile {percentile}: {min_value_indic}\")\n",
    "    print(f\"Maximum percentile {percentile}: {max_value_indic}\")\n",
    "\n",
    "    print(f\"Saved {new_filename} to {save_path}\")\n",
    "\n",
    "    return os.path.join(save_path, new_filename)  # Return path of processed file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38ea4aee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop through each subfolder (rcp26, rcp45, rcp85)\n",
    "for subfolder in subfolders:\n",
    "    print(subfolder)\n",
    "    folder_path = os.path.join(nc_files, subfolder)\n",
    "    save_subfolder = os.path.join(general_path, 'tempPercentiles', subfolder)\n",
    "\n",
    "    # Create the destination subfolder if it doesn't exist\n",
    "    os.makedirs(save_subfolder, exist_ok=True)\n",
    "\n",
    "    # Choose the time ranges based on the subfolder\n",
    "    if subfolder == 'historical':\n",
    "        time_ranges = historical_time_range\n",
    "    else:\n",
    "        time_ranges = rcp_time_ranges\n",
    "\n",
    "    # Initialize a dictionary to store processed files per threshold and time range\n",
    "    processed_files_by_threshold = {percentile: [] for percentile in percentiles}\n",
    "\n",
    "    # Loop through each NetCDF file in the subfolder\n",
    "    for file in os.listdir(folder_path):\n",
    "        file_path = os.path.join(folder_path, file)\n",
    "\n",
    "        # Check if it's a NetCDF file (usually ends with .nc)\n",
    "        if file.endswith('.nc'):\n",
    "            # Loop through the temperature thresholds\n",
    "            for percentile in percentiles:\n",
    "                # Loop through the defined time ranges\n",
    "                for start_year, end_year in time_ranges:\n",
    "                    print(f\"Processing Percentile {percentile} for time range {start_year}-{end_year}\")\n",
    "\n",
    "                    # Process and save the file with the new name for each time range\n",
    "                    processed_file_path = process_file(file_path, percentile, save_subfolder, start_year, end_year)\n",
    "\n",
    "print(\"Percentile calculation complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe67afa5",
   "metadata": {},
   "source": [
    "## Average the Cordex models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ae93260",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define base directory where your subfolders are located\n",
    "base_dir = \"/work/cmcc/dg07124/climax/indicators/cordex/tempPercentiles\"\n",
    "\n",
    "# Define the subfolders for each scenario\n",
    "subfolders = ['historical', 'rcp26', 'rcp45', 'rcp85']\n",
    "\n",
    "# Dictionary to store the file paths grouped by subfolder, threshold, and time period\n",
    "all_file_groups = {}\n",
    "\n",
    "# Create a new folder for averaged models\n",
    "averaged_results_dir = os.path.join(base_dir, 'avg_models')\n",
    "os.makedirs(averaged_results_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "326569d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop over each subfolder separately\n",
    "for subfolder in subfolders:\n",
    "    folder_path = os.path.join(base_dir, subfolder)\n",
    "\n",
    "    # Initialize a dictionary for each subfolder to store grouped files\n",
    "    file_groups = defaultdict(list)\n",
    "\n",
    "    # Loop over all files in the current subfolder\n",
    "    for file_name in os.listdir(folder_path):\n",
    "        if file_name.endswith(\".nc\"):\n",
    "            # Parse the threshold and time period from the filename\n",
    "            parts = file_name.split(\"_\")\n",
    "            threshold = parts[-2]  # e.g., 'above45'\n",
    "            time_period = parts[-1].replace(\".nc\", \"\")  # e.g., '2021-2050'\n",
    "\n",
    "            # Create the group key based on threshold and time period\n",
    "            group_key = (threshold, time_period)\n",
    "\n",
    "            # Save the file path under its group\n",
    "            file_path = os.path.join(folder_path, file_name)\n",
    "            file_groups[group_key].append(file_path)\n",
    "\n",
    "    # Save the grouped files for the current subfolder\n",
    "    all_file_groups[subfolder] = file_groups\n",
    "\n",
    "\n",
    "# Display the grouped file paths for verification\n",
    "for subfolder, file_groups in all_file_groups.items():\n",
    "    print(f\"\\nSubfolder: {subfolder}\")\n",
    "    for group_key, file_paths in file_groups.items():\n",
    "        threshold, time_period = group_key\n",
    "        print(f\"  Group: Threshold = {threshold}, Time Period = {time_period}\")\n",
    "        for file_path in file_paths:\n",
    "            print(f\"    - {file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "166a008f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# After verification, proceed with averaging the files for each group in each subfolder\n",
    "for subfolder, file_groups in all_file_groups.items():\n",
    "    for group_key, file_paths in file_groups.items():\n",
    "        print(\"---------------------------------------\")\n",
    "\n",
    "        # Unpack the group key (threshold and time_period)\n",
    "        threshold, time_period = group_key  # Correctly use time_period from the group\n",
    "\n",
    "        # Load all datasets in the group\n",
    "        datasets = [xr.open_dataset(fp) for fp in file_paths]\n",
    "        # Initialize a variable to hold the sum and count of valid datasets\n",
    "        summed = None\n",
    "        count = 0\n",
    "        invalid_files = []  # To store the filenames with NaN values only\n",
    "\n",
    "        for ds, fp in zip(datasets, file_paths):\n",
    "            # Select the 'tx_days_above' variable and check for non-NaN values\n",
    "            valid_data = ds['tasmax'].notnull()\n",
    "\n",
    "            # Print min and max value for the current dataset\n",
    "            min_value = ds['tasmax'].min().item()\n",
    "            max_value = ds['tasmax'].max().item()\n",
    "            print(f\"File: {fp}\")\n",
    "            print(f\"  Minimum value: {min_value}\")\n",
    "            print(f\"  Maximum value: {max_value}\")\n",
    "\n",
    "\n",
    "            if valid_data.any():  # If there's at least one valid value\n",
    "                if summed is None:\n",
    "                    summed = ds['tasmax'].copy()  # Initialize summed with the first valid dataset\n",
    "                else:\n",
    "                    summed += ds['tasmax']  # Add to the sum\n",
    "                count += 1  # Increment the count of valid datasets\n",
    "            else:\n",
    "                # If no valid data, add the file path to the invalid_files list\n",
    "                invalid_files.append(fp)\n",
    "\n",
    "        print(f\"Number of valid datasets {count}\")\n",
    "\n",
    "\n",
    "        # Print filenames that are fully NaN\n",
    "        if invalid_files:\n",
    "            print(\"Files with NaN values:\")\n",
    "            for invalid_file in invalid_files:\n",
    "                print(f\"  - {invalid_file}\")\n",
    "\n",
    "        # Compute the average across the datasets only if count > 0\n",
    "        if count > 0:\n",
    "            averaged = summed / count\n",
    "\n",
    "            # Define the subfolder for saving the averaged models for the current group\n",
    "            subfolder_avg_dir = os.path.join(averaged_results_dir, subfolder)\n",
    "            os.makedirs(subfolder_avg_dir, exist_ok=True)  # Create subfolder if it doesn't exist\n",
    "\n",
    "            # Define output file path using the subfolder, threshold, and time period\n",
    "            output_filename = f\"{subfolder}_avg_{threshold}_{time_period}.nc\"\n",
    "            output_path = os.path.join(subfolder_avg_dir, output_filename)\n",
    "\n",
    "            # Convert back to a dataset and save to a NetCDF file\n",
    "            averaged_ds = averaged.to_dataset(name='tasmax')\n",
    "\n",
    "            # Get the minimum and maximum values of the averaged dataset\n",
    "            min_value_avg = averaged_ds['tasmax'].min().item()\n",
    "            max_value_avg = averaged_ds['tasmax'].max().item()\n",
    "\n",
    "            # Print the results for the averaged dataset\n",
    "            print(f\"  Averaged Dataset: Minimum value: {min_value_avg}\")\n",
    "            print(f\"  Averaged Dataset: Maximum value: {max_value_avg}\")\n",
    "            averaged_ds.assign_coords({'lon' : ds.lon, 'lat':ds.lat})\n",
    "            averaged_ds.to_netcdf(output_path)  # Save the averaged dataset\n",
    "            print(f\"Averaged data saved to: {output_path}\")\n",
    "        else:\n",
    "            print(\"No valid datasets found for this group; skipping averaging.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a66751a",
   "metadata": {},
   "source": [
    "## Substract the future scenarios from the historical datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f68e1c82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the base directory containing the model subfoldersscenario\n",
    "base_dir = \"/work/cmcc/dg07124/climax/indicators/cordex/tempPercentiles\"\n",
    "scenario_dir = os.path.join(base_dir, 'avg_models')\n",
    "historical_dir = os.path.join(scenario_dir, 'historical')\n",
    "output_folder = os.path.join(base_dir, 'subtracted_rcps')\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "# Define the subfolders\n",
    "scenario_subfolders = ['rcp26', 'rcp45', 'rcp85']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d29bb4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "       \n",
    "# Loop over each scenario subfolder (rcp26, rcp45, rcp85)\n",
    "for scenario in scenario_subfolders:\n",
    "    scenario_folder = os.path.join(scenario_dir, scenario)\n",
    "\n",
    "    # Loop through each file in the scenario subfolder\n",
    "    for scenario_file in os.listdir(scenario_folder):\n",
    "        if scenario_file.endswith(\".nc\"):\n",
    "            # Extract the threshold from the scenario filename (e.g., 'above35' from 'rcp26_avg_above35_2021-2050.nc')\n",
    "            scenario_parts = scenario_file.split(\"_\")\n",
    "            scenario_threshold = scenario_parts[-2]  # e.g., 'above35'\n",
    "\n",
    "            # Find the corresponding historical file with the same threshold\n",
    "            matching_historical_file = None\n",
    "            for hist_file in os.listdir(historical_dir):\n",
    "                if hist_file.endswith(\".nc\"):\n",
    "                    hist_parts = hist_file.split(\"_\")\n",
    "                    historical_threshold = hist_parts[-2]  # e.g., 'above35'\n",
    "\n",
    "                    # Check if the thresholds match\n",
    "                    if scenario_threshold == historical_threshold:\n",
    "                        matching_historical_file = hist_file\n",
    "                        break  # Exit loop once a matching file is found\n",
    "\n",
    "            # If a matching historical file is found, proceed with subtraction\n",
    "            if matching_historical_file:\n",
    "                historical_file_path = os.path.join(historical_dir, matching_historical_file)\n",
    "\n",
    "                print(f\"Scenario file: {scenario_file}\")\n",
    "                print(f\"Historical file: {matching_historical_file}\")\n",
    "\n",
    "                # Load both scenario and historical datasets\n",
    "                scenario_ds = xr.open_dataset(os.path.join(scenario_folder, scenario_file))\n",
    "                historical_ds = xr.open_dataset(historical_file_path)\n",
    "\n",
    "                # Ensure both datasets contain the same variable ('tx_days_above') and then subtract\n",
    "                if 'tasmax' in scenario_ds and 'tasmax' in historical_ds:\n",
    "                    # Subtract historical from scenario\n",
    "                    diff = scenario_ds['tasmax'] - historical_ds['tasmax']\n",
    "\n",
    "                    # Save the difference to a new NetCDF file\n",
    "                    scenario_time_period = scenario_parts[-1].replace(\".nc\", \"\")  # Extract the time period\n",
    "                    percentile_thresh = scenario_threshold.replace('.', '')\n",
    "                    diff_filename = f\"diff_{scenario}_{percentile_thresh}_{scenario_time_period}.nc\"\n",
    "                    diff_filepath = os.path.join(output_folder, diff_filename)\n",
    "                    diff = diff.assign_coords({'lon' : historical_ds.lon, 'lat': historical_ds.lat})\n",
    "\n",
    "\n",
    "                    diff.to_dataset(name='tasmax').to_netcdf(diff_filepath)\n",
    "\n",
    "                    print(f\"Difference saved to: {diff_filepath}\")\n",
    "                else:\n",
    "                    print(f\"Variable 'tasmax' not found in one of the datasets.\")\n",
    "            else:\n",
    "                print(f\"Corresponding historical file not found for threshold: {scenario_threshold} in {scenario_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58a12200-0053-4e52-945e-844b6ad72543",
   "metadata": {},
   "source": [
    "## Contributors\n",
    "- Giuseppe Giugliano (giuseppe.giugliano@cmcc.it)\n",
    "- Carmela de Vivo (carmela.devivo@cmcc.it)\n",
    "- Daniela Quintero (daniela.quintero@cmcc.it)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
