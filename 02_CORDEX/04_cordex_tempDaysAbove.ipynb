{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "84851655-1184-4c17-a1b9-df42bdc98ca8",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Hazard assessment for Infrastructures using Euro-Cordex datasets\n",
    "## Calculation of the indicator \"Number of Days Abover 35°C, 40°C, and 45°C\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b378376d-aaf3-44b9-b331-bca0c125a749",
   "metadata": {
    "tags": []
   },
   "source": [
    "- See our [how to use risk workflows](https://handbook.climaax.eu/notebooks/workflows_how_to.html) page for information on how to run this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dd026bd",
   "metadata": {},
   "source": [
    "## Hazard assessment methodology\n",
    "We utilized outputs from 14 models within the EURO-CORDEX framework to evaluate hazards affecting infrastructure, in this notebook we used the daily maximum temperature as an indicator of the hazard. Our analysis included three Representative Concentration Pathways (RCPs): RCP2.6, RCP4.5, and RCP8.5. To structure the future projections, we divided the RCP scenarios into three distinct periods: 2021–2050, 2041–2070, and 2071–2100. Additionally, we used the historical period (1981–2010) as a baseline for comparison.\n",
    "\n",
    "For each period and model, the variation of the indicator \"number of days above 35°C, 40°C, and 45°C\" is calculated by comparing the future scenarios with the historical data. Finally, once the results for each model are obtained, the ensemble mean is computed. The results of these computations will be visualized in the notebook *04_cordex_tempDaysAbove.ipynb* to assess potential future hazards.\n",
    "\n",
    "## Limitation of the Euro-Cordex dataset\n",
    "The EURO-CORDEX (Coordinated Regional Climate Downscaling Experiment for Europe) project is a set of high-resolution regional climate projections for Europe, designed to support impact, adaptation, and vulnerability assessments under various climate change scenarios. The EURO-CORDEX integrate global climate model (GCM) outputs with regional climate models (RCMs), enabling the simulation of climatic patterns and extremes.\n",
    "The models explore different Representative Concentration Pathways (RCPs) from CMIP5 (RCP2.6, RCP4.5, RCP8.5) and Shared Socioeconomic Pathways (SSPs) from CMIP6 (SSP1-2.6, SSP5-8.5). The simulations cover historical periods (1950–2005) and future projections (2006–2100). These models are validated against observational data and reanalysis datasets.\n",
    "\n",
    "Some of the limitations:\n",
    "- EURO-CORDEX offers high-resolution data (typically 0.11° ~ 12.5 km and 0.44° ~ 50 km), it may still not fully capture localized phenomena such as urban heat islands, small-scale topographic effects, and small meteorological events.\n",
    "- Like all climate models, EURO-CORDEX RCMs and their driving GCMs exhibit biases compared to observed data, these Biases can vary regionally and seasonally. And may struggle to accurately simulate extreme weather events such as heatwaves, heavy precipitation, or storms.\n",
    "- While the dataset captures trends in extremes, very high thresholds (>45°C or >100 mm/day rainfall) may have higher uncertainty due to limited observational data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c7f97fe-2578-4ce9-b4c7-1fe40214f045",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Preparation work"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "796289e5",
   "metadata": {},
   "source": [
    "All the EURO-CORDEX models used in this workflow are freely available on copernicus C3S platform (https://cds.climate.copernicus.eu/datasets/projections-cordex-domains-single-levels?tab=overview), and Downloaded through the implemented API, the data were then processed to ensure that the grid type was consistent across all models and to fill any gaps in the dates. Here an example for one model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1679bebb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cdsapi\n",
    "\n",
    "dataset = \"projections-cordex-domains-single-levels\"\n",
    "request = {\n",
    "    \"domain\": \"europe\",\n",
    "    \"experiment\": \"historical\",\n",
    "    \"horizontal_resolution\": \"0_11_degree_x_0_11_degree\",\n",
    "    \"temporal_resolution\": \"daily_mean\",\n",
    "    \"variable\": [\"2m_air_temperature\"],\n",
    "    \"gcm_model\": \"ncc_noresm1_m\",\n",
    "    \"rcm_model\": \"mohc_hadrem3_ga7_05\",\n",
    "    \"ensemble_member\": \"r1i1p1\",\n",
    "    \"start_year\": [\"1981\"],\n",
    "    \"end_year\": [\"1985\"]\n",
    "}\n",
    "\n",
    "client = cdsapi.Client()\n",
    "client.retrieve(dataset, request).download()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd12ce07",
   "metadata": {},
   "source": [
    "### Select area of interest\n",
    "Before downloading the data, we will define the coordinates of the area of interest, for this workflow we selected the Italy region. Based on the shapefile of the country we will be able to clip the datasets for further processing, and display hazard and damage maps for this area."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6453335",
   "metadata": {},
   "source": [
    "### Load libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c445681b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import os\n",
    "import xarray as xr\n",
    "import xclim.indices\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "438e0df7",
   "metadata": {},
   "source": [
    "### Create the directory structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79e25ef6-9a3b-4d42-a2b5-d787c2b70878",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "nc_files = \"/work/cmcc/dg07124/climax/data/cordex/temp\"\n",
    "general_path = \"/work/cmcc/dg07124/climax/indicators/cordex/temp\"\n",
    "subfolders = ['historical','rcp26', 'rcp45', 'rcp85']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "262714f0-48e3-451c-a1c3-186a056f3286",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Temperature thresholds\n",
    "thresholds = ['35 C', '40 C', '45 C']\n",
    "\n",
    "# Time ranges to process\n",
    "rcp_time_ranges = [('2021', '2050'), ('2041', '2070'), ('2071', '2100')]\n",
    "historical_time_range = [('1981', '2010')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38ea4aee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to process each NetCDF file for a given time range\n",
    "def process_file(file_path, threshold, save_path, start_year, end_year):\n",
    "    print(\"---------------------------------------------------\")\n",
    "    print(f\"Processing {file_path} for time range {start_year}-{end_year}\")\n",
    "    ds = xr.open_dataset(file_path)\n",
    "\n",
    "    # Select daily max temperature for the given time range\n",
    "    ds_sliced = ds.sel(time=slice(start_year, end_year))\n",
    "    # Convert to Celcius\n",
    "    dailyMaxTemp = (ds_sliced['tasmax'] - 273.15).resample(time='D').max()\n",
    "    dailyMaxTemp.attrs['units'] = 'C'\n",
    "\n",
    "    # Get the minimum and maximum values\n",
    "    min_value = dailyMaxTemp.min(skipna=True).item()  # Convert to a scalar with .item()\n",
    "    max_value = dailyMaxTemp.max(skipna=True).item()\n",
    "\n",
    "    print(f\"Temp min value: {min_value}\")\n",
    "    print(f\"Temp max value: {max_value}\")\n",
    "\n",
    "\n",
    "    # Calculate the number of days above the threshold using xclim\n",
    "    with xclim.set_options(cf_compliance=\"log\"):\n",
    "        #NumbDaysAbove = xclim.atmos.tx_days_above(tasmax=dailyMaxTemp, thresh=threshold, freq=\"YS\")\n",
    "        NumbDaysAbove = xclim.indices.tx_days_above(dailyMaxTemp, thresh=threshold, freq='YS', op='>')\n",
    "\n",
    "    # Average over the time dimension (years in this case)\n",
    "    NumbDaysAbove_avg = NumbDaysAbove.mean(dim='time', skipna=True)\n",
    "\n",
    "    # Create the new filename with the time range and threshold information\n",
    "    filename = os.path.basename(file_path)  # Extract original filename\n",
    "    file_name_no_ext = os.path.splitext(filename)[0]  # Remove extension\n",
    "    number_threshold = re.findall(r'\\d+', threshold)[0]  # get only the number of the threshold\n",
    "    new_filename = f\"{file_name_no_ext}_above{number_threshold}_{start_year}-{end_year}.nc\"\n",
    "\n",
    "    # Save the result to the new file path\n",
    "    NumbDaysAbove_avg.to_netcdf(os.path.join(save_path, new_filename))\n",
    "\n",
    "    # Get the minimum and maximum values\n",
    "    min_value_indic = NumbDaysAbove_avg.min(skipna=True).item()  # Convert to a scalar with .item()\n",
    "    max_value_indic = NumbDaysAbove_avg.max(skipna=True).item()\n",
    "\n",
    "    # Print the results\n",
    "    print(f\"Minimum value days above: {min_value_indic}\")\n",
    "    print(f\"Maximum value days above: {max_value_indic}\")\n",
    "\n",
    "    print(f\"Saved {new_filename} to {save_path}\")\n",
    "\n",
    "    return os.path.join(save_path, new_filename)  # Return path of processed file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "787da358",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop through each subfolder (rcp26, rcp45, rcp85)\n",
    "for subfolder in subfolders:\n",
    "    print(subfolder)\n",
    "    folder_path = os.path.join(nc_files, subfolder)\n",
    "    save_subfolder = os.path.join(general_path, 'tempDaysAbove', subfolder)\n",
    "\n",
    "    # Create the destination subfolder if it doesn't exist\n",
    "    os.makedirs(save_subfolder, exist_ok=True)\n",
    "\n",
    "    # Choose the time ranges based on the subfolder\n",
    "    if subfolder == 'historical':\n",
    "        time_ranges = historical_time_range\n",
    "    else:\n",
    "        time_ranges = rcp_time_ranges\n",
    "\n",
    "    # Initialize a dictionary to store processed files per threshold and time range\n",
    "    processed_files_by_threshold = {threshold: [] for threshold in thresholds}\n",
    "\n",
    "    # Loop through each NetCDF file in the subfolder\n",
    "    for file in os.listdir(folder_path):\n",
    "        file_path = os.path.join(folder_path, file)\n",
    "\n",
    "        # Check if it's a NetCDF file (usually ends with .nc)\n",
    "        if file.endswith('.nc'):\n",
    "            # Loop through the temperature thresholds\n",
    "            for threshold in thresholds:\n",
    "                # Loop through the defined time ranges\n",
    "                for start_year, end_year in time_ranges:\n",
    "                    print(f\"Processing threshold {threshold} for time range {start_year}-{end_year}\")\n",
    "                    \n",
    "                    # Process and save the file with the new name for each time range\n",
    "                    processed_file_path = process_file(file_path, threshold, save_subfolder, start_year, end_year)\n",
    "\n",
    "print(\"Processing complete!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe67afa5",
   "metadata": {},
   "source": [
    "## Average the Cordex models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ae93260",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define base directory where the subfolders are located\n",
    "base_dir = \"/work/cmcc/dg07124/climax/indicators/cordex/tempDaysAbove\"\n",
    "\n",
    "# Define the subfolders for each scenario\n",
    "subfolders = ['historical', 'rcp26', 'rcp45', 'rcp85']\n",
    "\n",
    "# Dictionary to store the file paths grouped by subfolder, threshold, and time period\n",
    "all_file_groups = {}\n",
    "\n",
    "# Create a new folder for averaged models\n",
    "averaged_results_dir = os.path.join(base_dir, 'avg_models')\n",
    "os.makedirs(averaged_results_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "326569d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop over each subfolder separately\n",
    "for subfolder in subfolders:\n",
    "    folder_path = os.path.join(base_dir, subfolder)\n",
    "    \n",
    "    # Initialize a dictionary for each subfolder to store grouped files\n",
    "    file_groups = defaultdict(list)\n",
    "    \n",
    "    # Loop over all files in the current subfolder\n",
    "    for file_name in os.listdir(folder_path):\n",
    "        if file_name.endswith(\".nc\"):\n",
    "            # Parse the threshold and time period from the filename\n",
    "            parts = file_name.split(\"_\")\n",
    "            threshold = parts[-2]  # e.g., 'above45'\n",
    "            time_period = parts[-1].replace(\".nc\", \"\")  # e.g., '2021-2050'\n",
    "            \n",
    "            # Create the group key based on threshold and time period\n",
    "            group_key = (threshold, time_period)\n",
    "            \n",
    "            # Save the file path under its group\n",
    "            file_path = os.path.join(folder_path, file_name)\n",
    "            file_groups[group_key].append(file_path)\n",
    "    \n",
    "    # Save the grouped files for the current subfolder\n",
    "    all_file_groups[subfolder] = file_groups\n",
    "\n",
    "# Show the grouped file paths for verification\n",
    "for subfolder, file_groups in all_file_groups.items():\n",
    "    print(f\"\\nSubfolder: {subfolder}\")\n",
    "    for group_key, file_paths in file_groups.items():\n",
    "        threshold, time_period = group_key\n",
    "        print(f\"Group: Threshold = {threshold}, Time Period = {time_period}\")\n",
    "        for file_path in file_paths:\n",
    "            print(f\"    - {file_path}\")\n",
    "\n",
    "# After verification, proceed with averaging the files for each group in each subfolder\n",
    "for subfolder, file_groups in all_file_groups.items():\n",
    "    for group_key, file_paths in file_groups.items():\n",
    "        print(\"---------------------------------------\")\n",
    "        # print(group_key)\n",
    "        # print(file_paths)\n",
    "        \n",
    "        # Unpack the group key (threshold and time_period)\n",
    "        threshold, time_period = group_key  # Correctly use time_period from the group        \n",
    "        \n",
    "        # Load all datasets in the group\n",
    "        datasets = [xr.open_dataset(fp) for fp in file_paths]\n",
    "        # print(datasets)\n",
    "        # Initialize a variable to hold the sum and count of valid datasets\n",
    "        summed = None\n",
    "        count = 0\n",
    "        invalid_files = []  # To store the filenames with NaN values only\n",
    "\n",
    "        for ds, fp in zip(datasets, file_paths):\n",
    "            # Select the 'tx_days_above' variable and check for non-NaN values\n",
    "            valid_data = ds['tasmax'].notnull()\n",
    "\n",
    "            # Print min and max value for the current dataset\n",
    "            min_value = ds['tasmax'].min().item()\n",
    "            max_value = ds['tasmax'].max().item()\n",
    "            print(f\"File: {fp}\")\n",
    "            print(f\"Minimum value: {min_value}\")\n",
    "            print(f\"Maximum value: {max_value}\")\n",
    "\n",
    "\n",
    "            if valid_data.any():  # If there's at least one valid value\n",
    "                if summed is None:\n",
    "                    summed = ds['tasmax'].copy()  # Initialize summed with the first valid dataset\n",
    "                else:\n",
    "                    summed += ds['tasmax']  # Add to the sum\n",
    "                count += 1  # Increment the count of valid datasets\n",
    "            else:\n",
    "                # If no valid data, add the file path to the invalid_files list\n",
    "                invalid_files.append(fp)\n",
    "\n",
    "        print(f\"Number of valid datasets {count}\")\n",
    "\n",
    "        # Print filenames that are fully NaN\n",
    "        if invalid_files:\n",
    "            print(\"Files with NaN values:\")\n",
    "            for invalid_file in invalid_files:\n",
    "                print(f\"  - {invalid_file}\")\n",
    "\n",
    "        # Compute the average across the datasets only if count > 0\n",
    "        if count > 0:\n",
    "            averaged = summed / count\n",
    "            \n",
    "            # Define the subfolder for saving the averaged models for the current group\n",
    "            subfolder_avg_dir = os.path.join(averaged_results_dir, subfolder)\n",
    "            os.makedirs(subfolder_avg_dir, exist_ok=True)  # Create subfolder if it doesn't exist\n",
    "\n",
    "            # Define output file path using the subfolder, threshold, and time period\n",
    "            output_filename = f\"{subfolder}_avg_{threshold}_{time_period}.nc\"\n",
    "            output_path = os.path.join(subfolder_avg_dir, output_filename)\n",
    "\n",
    "            # Convert back to a dataset and save to a NetCDF file\n",
    "            averaged_ds = averaged.to_dataset(name='tasmax')\n",
    "\n",
    "            # Get the minimum and maximum values of the averaged dataset\n",
    "            min_value_avg = averaged_ds['tasmax'].min().item()\n",
    "            max_value_avg = averaged_ds['tasmax'].max().item()\n",
    "\n",
    "            # Print the results for the averaged dataset\n",
    "            print(f\"Averaged Dataset: Minimum value: {min_value_avg}\")\n",
    "            print(f\"Averaged Dataset: Maximum value: {max_value_avg}\")\n",
    "            averaged_ds.assign_coords({'lon' : ds.lon, 'lat':ds.lat})\n",
    "            averaged_ds.to_netcdf(output_path)  # Save the averaged dataset\n",
    "            print(f\"Averaged data saved to: {output_path}\")\n",
    "        else:\n",
    "            print(\"No valid datasets found for this group, skipping averaging.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a66751a",
   "metadata": {},
   "source": [
    "## Subtraction of the future scenarios from the historical datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f68e1c82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the base directory containing the models\n",
    "base_dir = \"/work/cmcc/dg07124/climax/indicators/cordex/tempDaysAbove\"\n",
    "scenario_dir = os.path.join(base_dir, 'avg_models')\n",
    "historical_dir = os.path.join(scenario_dir, 'historical')\n",
    "output_folder = os.path.join(base_dir, 'subtracted_rcps') # output\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "# Define the subfolders\n",
    "scenario_subfolders = ['rcp26', 'rcp45', 'rcp85']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d29bb4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop over each scenario subfolder (rcp26, rcp45, rcp85)\n",
    "for scenario in scenario_subfolders:\n",
    "    scenario_folder = os.path.join(scenario_dir, scenario)\n",
    "\n",
    "    # Loop through each file in the scenario subfolder\n",
    "    for scenario_file in os.listdir(scenario_folder):\n",
    "        if scenario_file.endswith(\".nc\"):\n",
    "            # Extract the threshold from the scenario filename (e.g., 'above35' from 'rcp26_avg_above35_2021-2050.nc')\n",
    "            scenario_parts = scenario_file.split(\"_\")\n",
    "            scenario_threshold = scenario_parts[-2]  # e.g., 'above35'\n",
    "\n",
    "            # Find the corresponding historical file with the same threshold\n",
    "            matching_historical_file = None\n",
    "            for hist_file in os.listdir(historical_dir):\n",
    "                if hist_file.endswith(\".nc\"):\n",
    "                    hist_parts = hist_file.split(\"_\")\n",
    "                    historical_threshold = hist_parts[-2]  # e.g., 'above35'\n",
    "                    \n",
    "                    # Check if the thresholds match\n",
    "                    if scenario_threshold == historical_threshold:\n",
    "                        matching_historical_file = hist_file\n",
    "                        break  # Exit loop once a matching file is found\n",
    "            \n",
    "            # If a matching historical file is found, proceed with subtraction\n",
    "            if matching_historical_file:\n",
    "                historical_file_path = os.path.join(historical_dir, matching_historical_file)\n",
    "\n",
    "                print(f\"Scenario file: {scenario_file}\")\n",
    "                print(f\"Historical file: {matching_historical_file}\")\n",
    "\n",
    "                # Load both scenario and historical datasets\n",
    "                scenario_ds = xr.open_dataset(os.path.join(scenario_folder, scenario_file))\n",
    "                historical_ds = xr.open_dataset(historical_file_path)\n",
    "\n",
    "                # Ensure both datasets contain the same variable ('tx_days_above') and then subtract\n",
    "                if 'tasmax' in scenario_ds and 'tasmax' in historical_ds:\n",
    "                    # Subtract historical from scenario\n",
    "                    diff = scenario_ds['tasmax'] - historical_ds['tasmax']\n",
    "\n",
    "                    # Save the difference to a new NetCDF file\n",
    "                    scenario_time_period = scenario_parts[-1].replace(\".nc\", \"\")  # Extract the time period\n",
    "                    diff_filename = f\"diff_{scenario}_{scenario_threshold}_{scenario_time_period}.nc\"\n",
    "                    diff_filepath = os.path.join(output_folder, diff_filename)\n",
    "                    diff = diff.assign_coords({'lon' : historical_ds.lon, 'lat': historical_ds.lat})\n",
    "                    \n",
    "                    # Save dataset to a local directory for future access\n",
    "                    diff.to_dataset(name='tasmax').to_netcdf(diff_filepath)\n",
    "\n",
    "                    print(f\"Difference saved to: {diff_filepath}\")\n",
    "                else:\n",
    "                    print(f\"Variable 'tasmax' not found in one of the datasets.\")\n",
    "            else:\n",
    "                print(f\"Corresponding historical file not found for threshold: {scenario_threshold} in {scenario_file}\")            \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58a12200-0053-4e52-945e-844b6ad72543",
   "metadata": {},
   "source": [
    "## Contributors\n",
    "- Giuseppe Giugliano (giuseppe.giugliano@cmcc.it)\n",
    "- Carmela de Vivo (carmela.devivo@cmcc.it)\n",
    "- Daniela Quintero (daniela.quintero@cmcc.it)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
