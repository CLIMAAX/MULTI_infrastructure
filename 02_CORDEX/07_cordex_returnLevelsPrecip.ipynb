{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "84851655-1184-4c17-a1b9-df42bdc98ca8",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Hazard assessment for Infrastructures using Euro-Cordex datasets\n",
    "## Calculation of the Return levels for the return periods of 10, 20, 30, 50, 100, 150 years"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b378376d-aaf3-44b9-b331-bca0c125a749",
   "metadata": {
    "tags": []
   },
   "source": [
    "- See our [how to use risk workflows](https://handbook.climaax.eu/notebooks/workflows_how_to.html) page for information on how to run this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dd026bd",
   "metadata": {},
   "source": [
    "## Hazard assessment methodology\n",
    "We utilized outputs from 14 models within the EURO-CORDEX framework to evaluate hazards affecting infrastructure, in this notebook we used the total daily precipitation as an indicator of the hazzard. Our analysis included three Representative Concentration Pathways (RCPs): RCP2.6, RCP4.5, and RCP8.5. To structure the future projections, we divided the RCP scenarios into three distinct periods: 2021–2050, 2041–2070, and 2071–2100. Additionally, we used the historical period (1981–2010) as a baseline for comparison.\n",
    "\n",
    "For each period and model, the variation of the indicator \"percentile associated to a return periods of 10, 20, 30, 50, 100, 150 years\" is calculated by comparing the future scenarios with the historical data. Finally, once the results for each model are obtained, the ensemble mean is computed. The results of these computations will be visualized in the notebook *07_cordex_returnLevelsPrecip_plots.ipynb* to assess potential future hazards.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd12ce07",
   "metadata": {},
   "source": [
    "### Select area of interest\n",
    "Before downloading the data, we will define the coordinates of the area of interest, for this workflow we selected the Italy region. Based on the shapefile of the country we will be able to clip the datasets for further processing, and display hazard and damage maps for this area."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6453335",
   "metadata": {},
   "source": [
    "### Load libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c445681b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cftime\n",
    "import datetime\n",
    "import os\n",
    "import xarray as xr\n",
    "import numpy as np\n",
    "from scipy.stats import gumbel_r"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "438e0df7",
   "metadata": {},
   "source": [
    "### Create the directory structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79e25ef6-9a3b-4d42-a2b5-d787c2b70878",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define paths and folders\n",
    "data_path = '/work/cmcc/dg07124/climax/data/cordex/precip'\n",
    "folders = ['historical', 'rcp26', 'rcp45', 'rcp85']\n",
    "avg_models_path = '/work/cmcc/dg07124/climax/indicators/cordex/avg_modelsPrecip'\n",
    "return_levels_path = '/work/cmcc/dg07124/climax/indicators/cordex/returnLevelsPrecip'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "262714f0-48e3-451c-a1c3-186a056f3286",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure output directories exist\n",
    "os.makedirs(avg_models_path, exist_ok=True)\n",
    "os.makedirs(return_levels_path, exist_ok=True)\n",
    "\n",
    "# Define time ranges for processing\n",
    "rcp_time_ranges = [('2021', '2050'), ('2041', '2070'), ('2071', '2100')]\n",
    "historical_time_range = [('1981', '2010')]\n",
    "\n",
    "# Define the return periods and their exceedance probabilities\n",
    "return_periods = np.array([10, 20, 30, 50, 100, 150])\n",
    "exceedance_probs = 1 - (1 / return_periods)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "787da358",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Process each folder\n",
    "for folder in folders:\n",
    "    folder_path = os.path.join(data_path, folder)\n",
    "    \n",
    "    # Load all NetCDF files in the folder and extract 'pr' variable to average\n",
    "    datasets = []\n",
    "    for filename in os.listdir(folder_path):\n",
    "        if filename.endswith('.nc'):\n",
    "            ds = xr.open_dataset(os.path.join(folder_path, filename))\n",
    "\n",
    "            # Check if time is already in a CFTime format\n",
    "            if isinstance(ds.indexes['time'][0], cftime.DatetimeNoLeap):\n",
    "                # Already in the desired 'noleap' format, no conversion needed\n",
    "                pass\n",
    "            elif isinstance(ds.indexes['time'][0], cftime._cftime.DatetimeAllLeap):\n",
    "                # Convert from 'all_leap' to 'noleap' by recreating the time index\n",
    "                start_date = ds.indexes['time'][0].strftime('%Y-%m-%d')\n",
    "                ds['time'] = xr.cftime_range(\n",
    "                    start=start_date,\n",
    "                    periods=len(ds['time']),\n",
    "                    freq='D',\n",
    "                    calendar='noleap'\n",
    "                )\n",
    "            else:\n",
    "                # Assume 'time' is numpy.datetime64; convert to 'noleap' CFTimeIndex\n",
    "                start_date = str(ds['time'].values[0].astype('M8[D]'))  # Convert to ISO string\n",
    "                ds['time'] = xr.cftime_range(\n",
    "                    start=start_date,\n",
    "                    periods=len(ds['time']),\n",
    "                    freq='D',\n",
    "                    calendar='noleap'\n",
    "                )\n",
    "\n",
    "\n",
    "            # Convert 'pr' to mm/day by multiplying by 86400\n",
    "            pr_mm_day = ds['pr'] * 86400\n",
    "            pr_mm_day.attrs['units'] = 'mm/day'  # Update units attribute\n",
    "            datasets.append(pr_mm_day)              \n",
    "            # datasets.append(ds['pr'])  # Assuming 'pr' is the precipitation variable name\n",
    "\n",
    "    # Stack datasets along a new 'model' dimension and compute the mean across models\n",
    "    combined_ds = xr.concat(datasets, dim='model').mean(dim='model')\n",
    "\n",
    "    # Ensure latitude and longitude coordinates are retained\n",
    "    if 'lat' in datasets[0].coords and 'lon' in datasets[0].coords:\n",
    "        combined_ds = combined_ds.assign_coords({\n",
    "            'lat': datasets[0]['lat'],  # Retain the 2D latitude coordinates\n",
    "            'lon': datasets[0]['lon']   # Retain the 2D longitude coordinates\n",
    "        })\n",
    "\n",
    "    # Save the averaged dataset\n",
    "    avg_file_path = os.path.join(avg_models_path, f\"{folder}_avgPrecip.nc\")\n",
    "    combined_ds.to_netcdf(avg_file_path)\n",
    "    print(f\"Averaged NetCDF saved to: {avg_file_path}\")\n",
    "\n",
    "\n",
    "    # Determine the time ranges to use\n",
    "    time_ranges = historical_time_range if folder == 'historical' else rcp_time_ranges\n",
    "\n",
    "\n",
    "    # Process each specified time range\n",
    "    for start_year, end_year in time_ranges:\n",
    "        # Subset data to the specified time range\n",
    "        ds_subset = combined_ds.sel(time=slice(start_year, end_year))\n",
    "        \n",
    "        # Calculate the annual maximum precipitation for each year\n",
    "        annual_max_precip = ds_subset.resample(time='YE').max()\n",
    "        \n",
    "        return_levels_ds = xr.Dataset()\n",
    "        \n",
    "        if 'lat' in datasets[0].coords and 'lon' in datasets[0].coords:\n",
    "            return_levels_ds = return_levels_ds.assign_coords({\n",
    "                'lat': datasets[0]['lat'],  # Retain 2D latitude coordinates\n",
    "                'lon': datasets[0]['lon']   # Retain 2D longitude coordinates\n",
    "            })\n",
    "\n",
    "\n",
    "        for y_coord in range(annual_max_precip.sizes['y']):\n",
    "            #for x_coord in annual_max_precip['x']:\n",
    "            for x_coord in range(annual_max_precip.sizes['x']):\n",
    "                # Extract the annual maximum series for the current grid cell\n",
    "                annual_max_values = annual_max_precip.sel(x=x_coord, y=y_coord).values\n",
    "                annual_max_values = annual_max_values[~np.isnan(annual_max_values)]  # Remove NaNs\n",
    "                \n",
    "                if len(annual_max_values) > 0:\n",
    "                    # Fit the Gumbel distribution to the annual maxima for this grid cell\n",
    "                    loc, scale = gumbel_r.fit(annual_max_values)\n",
    "                    \n",
    "                    # Calculate return levels for each return period\n",
    "                    return_levels = gumbel_r.ppf(exceedance_probs, loc, scale)\n",
    "                    \n",
    "                    # Store return levels in the dataset\n",
    "                    for rp, rl in zip(return_periods, return_levels):\n",
    "                        return_period_label = f\"return_period_{rp}_y\"\n",
    "                        if return_period_label not in return_levels_ds:\n",
    "                            return_levels_ds[return_period_label] = xr.full_like(annual_max_precip.isel(time=0), np.nan)\n",
    "                        return_levels_ds[return_period_label][y_coord, x_coord] = rl\n",
    "        \n",
    "        # Save the return levels dataset for the current time range\n",
    "        return_levels_file_path = os.path.join(\n",
    "            return_levels_path, f\"{folder}_return_levels_{start_year}_{end_year}.nc\"\n",
    "        )\n",
    "        return_levels_ds.to_netcdf(return_levels_file_path)\n",
    "        print(f\"Return levels NetCDF saved to: {return_levels_file_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe67afa5",
   "metadata": {},
   "source": [
    "## Anomalies calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ae93260",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define paths\n",
    "return_levels_path = '/work/cmcc/dg07124/climax/indicators/cordex/returnLevelsPrecip'\n",
    "output_path = '/work/cmcc/dg07124/climax/indicators/cordex/returnLevels_anomalies'\n",
    "os.makedirs(output_path, exist_ok=True)\n",
    "\n",
    "# Define scenarios and time ranges\n",
    "scenarios = ['rcp26', 'rcp45', 'rcp85']\n",
    "time_ranges = [('2021', '2050'), ('2041', '2070'), ('2071', '2100')]\n",
    "historical_file = os.path.join(return_levels_path, \"historical_return_levels_1981_2010.nc\")\n",
    "\n",
    "# Load the historical return levels dataset\n",
    "historical_ds = xr.open_dataset(historical_file)\n",
    "historical_ds_var =historical_ds['return_period_10_y']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a354a976",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure lat and lon are present\n",
    "if 'lat' not in historical_ds.coords or 'lon' not in historical_ds.coords:\n",
    "    raise ValueError(\"Historical dataset is missing 'lat' or 'lon' coordinates\")\n",
    "\n",
    "print(historical_ds['lat'].values)\n",
    "print(historical_ds['lon'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "326569d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop through each scenario and time range\n",
    "for scenario in scenarios:\n",
    "    for start_year, end_year in time_ranges:\n",
    "        # Construct file path for the scenario return levels dataset\n",
    "        scenario_file = os.path.join(return_levels_path, f\"{scenario}_return_levels_{start_year}_{end_year}.nc\")\n",
    "        if not os.path.exists(scenario_file):\n",
    "            print(f\"File not found: {scenario_file}\")\n",
    "            continue\n",
    "        \n",
    "        # Load the scenario return levels dataset\n",
    "        scenario_ds = xr.open_dataset(scenario_file)\n",
    "        \n",
    "        # Calculate the anomaly (scenario - historical)\n",
    "        anomaly_ds = scenario_ds - historical_ds\n",
    "        anomaly_ds.attrs['description'] = f\"Return level anomalies for {scenario} ({start_year}-{end_year}) relative to historical (1981-2010)\"\n",
    "\n",
    "        # Assign lat and lon correctly to anomaly_ds\n",
    "        anomaly_ds = anomaly_ds.assign_coords({\n",
    "            'lat': (('y', 'x'), historical_ds['lat'].values),  # Assign lat as 2D coordinates\n",
    "            'lon': (('y', 'x'), historical_ds['lon'].values)   # Assign lon as 2D coordinates\n",
    "        })\n",
    "\n",
    "\n",
    "        # Calculate the percentage change ((scenario - historical) / historical) * 100\n",
    "        percentage_change_ds = (anomaly_ds / historical_ds) * 100\n",
    "        percentage_change_ds.attrs['description'] = f\"Percentage change in return levels for {scenario} ({start_year}-{end_year}) relative to historical (1981-2010)\"\n",
    "        \n",
    "        # Assign lat and lon correctly to percentage_change_ds\n",
    "        percentage_change_ds = percentage_change_ds.assign_coords({\n",
    "            'lat': (('y', 'x'), historical_ds['lat'].values),  # Assign lat as 2D coordinates\n",
    "            'lon': (('y', 'x'), historical_ds['lon'].values)   # Assign lon as 2D coordinates\n",
    "        })\n",
    "\n",
    "        # Define file paths for saving\n",
    "        anomaly_file = os.path.join(output_path, f\"{scenario}_anomaly_return_levels_{start_year}_{end_year}.nc\")\n",
    "        percentage_change_file = os.path.join(output_path, f\"{scenario}_percentage_change_return_levels_{start_year}_{end_year}.nc\")\n",
    "       \n",
    "        # Save the anomaly and percentage change datasets\n",
    "        anomaly_ds.to_netcdf(anomaly_file)\n",
    "        percentage_change_ds.to_netcdf(percentage_change_file)\n",
    "        \n",
    "        print(f\"Anomaly file saved to: {anomaly_file}\")\n",
    "        print(f\"Percentage change file saved to: {percentage_change_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58a12200-0053-4e52-945e-844b6ad72543",
   "metadata": {},
   "source": [
    "## Contributors\n",
    "- Giuseppe Giugliano (giuseppe.giugliano@cmcc.it)\n",
    "- Carmela de Vivo (carmela.devivo@cmcc.it)\n",
    "- Daniela Quintero (daniela.quintero@cmcc.it)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
