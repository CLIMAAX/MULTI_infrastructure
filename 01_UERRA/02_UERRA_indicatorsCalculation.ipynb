{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hazard assessment methodology\n",
    "The impact of the single hazard (e.g. extreme precipitation, heat waves) can be evaluated on the different assets of the infrastructure, for example on the terminal, runway, taxiways, parking areas etc.\n",
    "\n",
    "The idea is that this script can be run by expert users to understand and potentially modify how the hazard map data is calculated for use in the risk toolbox CLIMAAX.  \n",
    "Describe where the data can be found, is there an API to download it or the files can be downloaded from some data repository. Provide a link to the repository (as DOI if possible).\n",
    "\n",
    "The data used in this section can be found on Copernicu Climate Change Data Store (C3S) and they are related to UERRA SURFACE-MESCAN (https://cds.climate.copernicus.eu/datasets/reanalysis-uerra-europe-single-levels?tab=download)\n",
    "on single level. The automathic dowload was made through Copernicus API (cdsapi python packages) both for temperature and precipitation data. \n",
    "\n",
    "The UERRA dataset was chosen for it's horizontal and temporal resolutio, providing a quite high resolution with continuos measurment. On the opposite, the 5.5km horizontal resolution could be to course for airport hazard assessment in order to evaluate this type of indicator for single airport parts (e.g taxiways and runway)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load libraries\n",
    "xclim is a Python library tailored for climate services, offering a wide range of tools to compute climate-related indicators. In this notebook, it is the primary library used to calculate the climate indicator \"number of days above a threshold temperature\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from useful_functions import get_subfiles\n",
    "\n",
    "import os\n",
    "import xarray as xr\n",
    "import xclim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the directory structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nc_files = \"../data_conv\"\n",
    "general_path = '../indicators/uerra'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process the data\n",
    "This script processes the temperature and precipitation files from UERRA within the folder 'nc_files' and performs the calculation of some indicators. Specifically:\n",
    "\n",
    "- Avoids recomputation by checking if output files already exist.\n",
    "- Calculates the number of days exceeding specific temperature thresholds (35, 40 and 45 Celcius).\n",
    "- Calculates the percentiles for both temperature and precipitation. The code uses xarray for handling NetCDF files, and xclim to compute climate indices like the number of hot days and percentiles. For each file, the results are saved as new NetCDF files in a specified directory.\n",
    "- it handles missing values to avoid errors in the calculations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop through all files in the folder\n",
    "for file, file_path in get_subfiles(nc_files):\n",
    "    if file.endswith(\".nc\"):  # Check if file is .nc\n",
    "        # Load the dataset with lazy loading and chunking\n",
    "        ds = xr.open_dataset(file_path) #, chunks={'time': 'auto'})\n",
    "\n",
    "        try:\n",
    "            print(file)\n",
    "            if \"t2m\" in ds.data_vars:\n",
    "                print('Temperature')\n",
    "                # Compute maximum daily temperature and convert to Celsius\n",
    "                dailyMaxTemp = ds['t2m'].resample(time='D').max()\n",
    "                dailyMaxTemp.attrs['units'] = 'C'  # Set the units\n",
    "\n",
    "                # -------- Define the output files for Number of Days Above Thresholds  --------\n",
    "                output_files_numDays = {\n",
    "                    'NumbDaysAbove30.nc': \"30 C\",\n",
    "                    'NumbDaysAbove35.nc': \"35 C\",\n",
    "                    'NumbDaysAbove40.nc': \"40 C\",\n",
    "                    'NumbDaysAbove45.nc': \"45 C\",\n",
    "                }\n",
    "\n",
    "                for fname, threshold in output_files_numDays.items():\n",
    "                    output_path = os.path.join(general_path, fname)\n",
    "\n",
    "                    if os.path.exists(output_path):\n",
    "                        print(f\"The file {fname} already exists. Skipping calculation.\")\n",
    "                    else:\n",
    "                        # Compute the number of days above the threshold\n",
    "                        with xclim.set_options(cf_compliance=\"log\"):\n",
    "                            NumbDaysAbove = xclim.atmos.tx_days_above(tasmax=dailyMaxTemp, thresh=threshold, freq=\"YS\")\n",
    "                            NumbDaysAbove_avg = NumbDaysAbove.mean(dim='time', skipna=True)\n",
    "\n",
    "                            # Save it\n",
    "                            NumbDaysAbove_avg.to_netcdf(output_path)\n",
    "                            print(f\"Saved {fname} to {output_path}.\")\n",
    "\n",
    "                # -------- Define the output files for Percentiles ------------\n",
    "                output_files_percent = {\n",
    "                    'Temp_P95.nc': 0.95,\n",
    "                    'Temp_P999.nc': 0.999,\n",
    "                }\n",
    "\n",
    "                for fname_percent, percentile in output_files_percent.items():\n",
    "                    print(fname_percent, percentile)\n",
    "                    output_path_percent = os.path.join(general_path, fname_percent)\n",
    "\n",
    "                    if os.path.exists(output_path_percent):\n",
    "                        print(f\"The file {fname_percent} already exists. Skipping calculation.\")\n",
    "                    else:\n",
    "                        # Calculate the percentiles across all time steps\n",
    "                        dailyMaxTemp_nonan = dailyMaxTemp.dropna(dim='time', how='all')\n",
    "                        calc_percentile = dailyMaxTemp_nonan.quantile(percentile, dim='time')\n",
    "\n",
    "                        # Save it\n",
    "                        calc_percentile.to_netcdf(output_path_percent)\n",
    "                        print(f\"Saved {percentile * 100}th percentile to {output_path_percent}.\")\n",
    "\n",
    "            elif \"tp\" in ds.data_vars:\n",
    "                dailyTotPrep = ds['tp']\n",
    "                # -------- Define the output files for Percentiles ------------\n",
    "                output_files_percent_prec = {\n",
    "                    'Precip_P95.nc': 0.99,\n",
    "                    'Precip_P995.nc': 0.995,\n",
    "                    'Precip_P999.nc': 0.999}\n",
    "\n",
    "                for fname_percent_prep, percentile_prep in output_files_percent_prec.items():\n",
    "                    print(fname_percent_prep, percentile_prep)\n",
    "                    output_path_percent_prep = os.path.join(general_path, fname_percent_prep)\n",
    "\n",
    "                    if os.path.exists(output_path_percent_prep):\n",
    "                        print(f\"The file {fname_percent_prep} already exists. Skipping calculation.\")\n",
    "                    else:\n",
    "                        # Calculate the percentiles across all time steps\n",
    "                        dailyTotPrep_nonan = dailyTotPrep.dropna(dim='time', how='all')\n",
    "                        calc_percentile_prep = dailyTotPrep_nonan.quantile(percentile_prep, dim='time')\n",
    "\n",
    "                        # Save it\n",
    "                        calc_percentile_prep.to_netcdf(output_path_percent_prep)\n",
    "                        print(f\"Saved {percentile_prep * 100}th percentile to {output_path_percent_prep}.\")\n",
    "        finally:\n",
    "            # close the dataset to free memory\n",
    "            ds.close()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "climax",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
