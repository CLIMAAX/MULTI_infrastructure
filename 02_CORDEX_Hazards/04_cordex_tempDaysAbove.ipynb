{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "84851655-1184-4c17-a1b9-df42bdc98ca8",
   "metadata": {
    "tags": []
   },
   "source": [
    "# **Hazard assessment for Infrastructures using Euro-Cordex datasets**\n",
    "## Calculation of the indicator \"Number of Days Abover 35°C, 40°C, and 45°C\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b378376d-aaf3-44b9-b331-bca0c125a749",
   "metadata": {
    "tags": []
   },
   "source": [
    "- See our [how to use risk workflows](https://handbook.climaax.eu/notebooks/workflows_how_to.html) page for information on how to run this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dd026bd",
   "metadata": {},
   "source": [
    "## **Hazard assessment methodology**\n",
    "We utilized outputs from 14 models within the EURO-CORDEX framework to evaluate hazards affecting infrastructure, in this notebook we used the daily maximum temperature as an indicator of the hazzard. Our analysis included three Representative Concentration Pathways (RCPs): RCP2.6, RCP4.5, and RCP8.5. To structure the future projections, Each RCP scenario was analyzed over three distinct future timeframes: 2021–2050, 2041–2070, and 2071–2100. Additionally, we used the historical period (1981–2010) as a baseline for comparison to evaluate changes in climate hazards over time.\n",
    "\n",
    "### Temperature Analysis\n",
    "For daily maximum temperature, we calculated the number of days exceeding thresholds of 35°C, 40°C, and 45°C for each model, scenario, and time period. These calculations were performed for both the historical and future RCP scenarios. To quantify changes, we calculated anomalies by subtracting, for each individual model, the historical dataset from its corresponding future projection (RCP2.6, RCP4.5, and RCP8.5).\n",
    "\n",
    "To account for uncertainties and provide a robust projection, we computed the average across all 14 models for each indictor threshold of temperature, scenario, and time period. The ensemble averaging process involved aggregating anomalies for all models and then calculating the mean, yielding a single representative dataset for each RCP scenario and timeframe.\n",
    "\n",
    "\n",
    "\n",
    "## **Limitation of the Euro-Cordex dataset**\n",
    "The EURO-CORDEX (Coordinated Regional Climate Downscaling Experiment for Europe) project is a set of high-resolution regional climate projections for Europe, designed to support impact, adaptation, and vulnerability assessments under various climate change scenarios. The EURO-CORDEX integrate global climate model (GCM) outputs with regional climate models (RCMs), enabling the simulation of climatic patterns and extremes. The models explore different Representative Concentration Pathways (RCPs) from CMIP5 (RCP2.6, RCP4.5, RCP8.5) and Shared Socioeconomic Pathways (SSPs) from CMIP6 (SSP1-2.6, SSP5-8.5). The simulations cover historical periods (1950–2005) and future projections (2006–2100). These models are validated against observational data and reanalysis datasets\n",
    "\n",
    "Some of the limitations:\n",
    "- EURO-CORDEX offers high-resolution data (typically 0.11° ~ 12.5 km and 0.44° ~ 50 km), it may still not fully capture localized phenomena such as urban heat islands, small-scale topographic effects, and small meteorological events.\n",
    "- Like all climate models, EURO-CORDEX RCMs and their driving GCMs exhibit biases compared to observed data, these Biases can vary regionally and seasonally. And may struggle to accurately simulate extreme weather events such as heatwaves, heavy precipitation, or storms.\n",
    "- While the dataset captures trends in extremes, very high thresholds (>45°C or >100 mm/day rainfall) may have higher uncertainty due to limited observational data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c7f97fe-2578-4ce9-b4c7-1fe40214f045",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Preparation work"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "796289e5",
   "metadata": {},
   "source": [
    "All the EURO-CORDEX models used in this workflow are freely available on copernicus C3S platform (https://cds.climate.copernicus.eu/datasets/projections-cordex-domains-single-levels?tab=overview), and Downloaded through the implemented API, the data were then processed to ensure that the grid type was consistent across all models and to fill any gaps in the dates. Here an example for one model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd12ce07",
   "metadata": {},
   "source": [
    "### Select area of interest\n",
    "Before downloading the data, we will define the coordinates of the area of interest, for this workflow we selected the Italy region. Based on the shapefile of the country we will be able to clip the datasets for further processing, and display hazard and damage maps for this area."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6453335",
   "metadata": {},
   "source": [
    "### Load libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c445681b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import os\n",
    "import xarray as xr\n",
    "import xclim.indices\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "438e0df7",
   "metadata": {},
   "source": [
    "### Create the directory structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79e25ef6-9a3b-4d42-a2b5-d787c2b70878",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "nc_files = \"/climax/data/cordex/temp\"\n",
    "general_path = \"/climax/indicators/cordex/temp\"\n",
    "subfolders = ['historical','rcp26', 'rcp45', 'rcp85']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "262714f0-48e3-451c-a1c3-186a056f3286",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Temperature thresholds\n",
    "thresholds = ['35 C', '40 C', '45 C']\n",
    "\n",
    "# Time ranges to process\n",
    "rcp_time_ranges = [('2021', '2050'), ('2041', '2070'), ('2071', '2100')]\n",
    "historical_time_range = [('1981', '2010')]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b346e3f",
   "metadata": {},
   "source": [
    "### **Calculation of the indicator \"Number of Days Above 35 °C, 40 °C, 45 °C for each model for the historical dataset and the future scenarios**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38ea4aee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to process each NetCDF file for a given time range\n",
    "def process_file(file_path, threshold, save_path, start_year, end_year):\n",
    "    print(\"---------------------------------------------------\")\n",
    "    print(f\"Processing {file_path} for time range {start_year}-{end_year}\")\n",
    "    ds = xr.open_dataset(file_path)\n",
    "\n",
    "    # Select daily max temperature for the given time range\n",
    "    ds_sliced = ds.sel(time=slice(start_year, end_year))\n",
    "    # Convert to Celcius\n",
    "    dailyMaxTemp = (ds_sliced['tasmax'] - 273.15).resample(time='D').max()\n",
    "    dailyMaxTemp.attrs['units'] = 'C'\n",
    "\n",
    "    # Get the minimum and maximum values\n",
    "    min_value = dailyMaxTemp.min(skipna=True).item()  # Convert to a scalar with .item()\n",
    "    max_value = dailyMaxTemp.max(skipna=True).item()\n",
    "\n",
    "    print(f\"Temp min value: {min_value}\")\n",
    "    print(f\"Temp max value: {max_value}\")\n",
    "\n",
    "\n",
    "    # Calculate the number of days above the threshold using xclim\n",
    "    with xclim.set_options(cf_compliance=\"log\"):\n",
    "        #NumbDaysAbove = xclim.atmos.tx_days_above(tasmax=dailyMaxTemp, thresh=threshold, freq=\"YS\")\n",
    "        NumbDaysAbove = xclim.indices.tx_days_above(dailyMaxTemp, thresh=threshold, freq='YS', op='>')\n",
    "\n",
    "    # Average over the time dimension (years in this case)\n",
    "    NumbDaysAbove_avg = NumbDaysAbove.mean(dim='time', skipna=True)\n",
    "\n",
    "    # Create the new filename with the time range and threshold information\n",
    "    filename = os.path.basename(file_path)  # Extract original filename\n",
    "    file_name_no_ext = os.path.splitext(filename)[0]  # Remove extension\n",
    "    number_threshold = re.findall(r'\\d+', threshold)[0]  # get only the number of the threshold\n",
    "    new_filename = f\"{file_name_no_ext}_above{number_threshold}_{start_year}-{end_year}.nc\"\n",
    "\n",
    "    # Save the result to the new file path\n",
    "    NumbDaysAbove_avg.to_netcdf(os.path.join(save_path, new_filename))\n",
    "\n",
    "    # Get the minimum and maximum values\n",
    "    min_value_indic = NumbDaysAbove_avg.min(skipna=True).item()  # Convert to a scalar with .item()\n",
    "    max_value_indic = NumbDaysAbove_avg.max(skipna=True).item()\n",
    "\n",
    "    # Print the results\n",
    "    print(f\"Minimum value days above: {min_value_indic}\")\n",
    "    print(f\"Maximum value days above: {max_value_indic}\")\n",
    "\n",
    "    print(f\"Saved {new_filename} to {save_path}\")\n",
    "\n",
    "    return os.path.join(save_path, new_filename)  # Return path of processed file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "787da358",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop through each subfolder (rcp26, rcp45, rcp85)\n",
    "for subfolder in subfolders:\n",
    "    print(subfolder)\n",
    "    folder_path = os.path.join(nc_files, subfolder)\n",
    "    save_subfolder = os.path.join(general_path, 'tempDaysAbove', subfolder)\n",
    "\n",
    "    # Create the destination subfolder if it doesn't exist\n",
    "    os.makedirs(save_subfolder, exist_ok=True)\n",
    "\n",
    "    # Choose the time ranges based on the subfolder\n",
    "    if subfolder == 'historical':\n",
    "        time_ranges = historical_time_range\n",
    "    else:\n",
    "        time_ranges = rcp_time_ranges\n",
    "\n",
    "    # Initialize a dictionary to store processed files per threshold and time range\n",
    "    processed_files_by_threshold = {threshold: [] for threshold in thresholds}\n",
    "\n",
    "    # Loop through each NetCDF file in the subfolder\n",
    "    for file in os.listdir(folder_path):\n",
    "        file_path = os.path.join(folder_path, file)\n",
    "\n",
    "        # Check if it's a NetCDF file (usually ends with .nc)\n",
    "        if file.endswith('.nc'):\n",
    "            # Loop through the temperature thresholds\n",
    "            for threshold in thresholds:\n",
    "                # Loop through the defined time ranges\n",
    "                for start_year, end_year in time_ranges:\n",
    "                    print(f\"Processing threshold {threshold} for time range {start_year}-{end_year}\")\n",
    "                    \n",
    "                    # Process and save the file with the new name for each time range\n",
    "                    processed_file_path = process_file(file_path, threshold, save_subfolder, start_year, end_year)\n",
    "\n",
    "print(\"Processing complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe67afa5",
   "metadata": {},
   "source": [
    "### **Anomly Calculation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ae93260",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directories\n",
    "historical_dir = \"/climax/indicators/cordex2/tempdays/tempDaysAbove/historical\"\n",
    "rcp26_dir = \"/indicators/cordex/tempdays/tempDaysAbove/rcp6\"\n",
    "rcp45_dir = \"/indicators/cordex/tempdays/tempDaysAbove/rcp45\"\n",
    "rcp85_dir = \"/indicators/cordex/tempdays/tempDaysAbove/rcp85\"\n",
    "\n",
    "output_dir = \"/climax/indicators/cordex/tempdays/tempDaysAbove/anomalies\"\n",
    "\n",
    "# Create the output directory if does not exists\n",
    "os.makedirs(output_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bef5a79d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to parse filenames and extract key components\n",
    "def parse_filename(filename):\n",
    "    pattern = r\"tasmax_EUR-11_([A-Za-z0-9\\-]+)_(historical|rcp\\d{2})_r\\d+i\\d+p\\d+_([A-Za-z0-9\\-]+)_v\\d+_day_\\d{8}_(above\\d+)_([\\d-]+)\\.nc\"\n",
    "    match = re.match(pattern, filename)\n",
    "    if match:\n",
    "        model = match.group(1)\n",
    "        scenario = match.group(2)\n",
    "        rcm = match.group(3)\n",
    "        threshold = match.group(4)\n",
    "        time_period = match.group(5)\n",
    "        return model, scenario, rcm, threshold, time_period\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83f9eaec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load filenames into dictionaries\n",
    "historical_files = {parse_filename(f): os.path.join(historical_dir, f) for f in os.listdir(historical_dir) if f.endswith(\".nc\")}\n",
    "rcp26_files = {parse_filename(f): os.path.join(rcp26_dir, f) for f in os.listdir(rcp26_dir) if f.endswith(\".nc\")}\n",
    "rcp45_files = {parse_filename(f): os.path.join(rcp45_dir, f) for f in os.listdir(rcp45_dir) if f.endswith(\".nc\")}\n",
    "rcp85_files = {parse_filename(f): os.path.join(rcp85_dir, f) for f in os.listdir(rcp85_dir) if f.endswith(\".nc\")}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73342178",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to perform subtraction with xarray\n",
    "def subtract_and_save(historical_file, future_file, output_file):\n",
    "    # Load the datasets using xarray\n",
    "    historical_ds = xr.open_dataset(historical_file)\n",
    "    future_ds = xr.open_dataset(future_file)\n",
    "    print(f\"Historical file {historical_file}\")\n",
    "    print(f\"Scenario file: {future_file}\")\n",
    "\n",
    "    # Perform subtraction for the 'tasmax' variable\n",
    "    diff = future_ds['tasmax'] - historical_ds['tasmax']\n",
    "\n",
    "    # Create a new dataset with the diff\n",
    "    diff_ds = diff.to_dataset(name='tasmax')\n",
    "\n",
    "    # Copy attributes if needed\n",
    "    diff_ds.attrs = future_ds.attrs\n",
    "\n",
    "    # Save the result\n",
    "    diff_ds.to_netcdf(output_file)\n",
    "    print(f\"Saved: {output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "326569d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Match files and process\n",
    "for key, hist_file in historical_files.items():\n",
    "    print(key)\n",
    "    print(hist_file)\n",
    "    print(\"----------------------------------------\")\n",
    "    if key is None:\n",
    "        continue\n",
    "    model, _, rcm, threshold, _ = key\n",
    "\n",
    "    # Iterate over all future scenarios\n",
    "    for future_files in [rcp26_files, rcp45_files, rcp85_files]:\n",
    "        for f_key, fut_file in future_files.items():\n",
    "            if f_key is None:\n",
    "                continue\n",
    "            fut_model, scenario, fut_rcm, fut_threshold, time_period = f_key\n",
    "\n",
    "            # Match by model, threshold, and RCM\n",
    "            if model == fut_model and rcm == fut_rcm and threshold == fut_threshold:\n",
    "                output_filename = f\"tasmax_EUR-11_{model}_{scenario}_diff_{rcm}_{threshold}_{time_period}.nc\"\n",
    "                output_path = os.path.join(output_dir, output_filename)\n",
    "                subtract_and_save(hist_file, fut_file, output_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a66751a",
   "metadata": {},
   "source": [
    "### **Outputs and Hazard Assessment**\n",
    "\n",
    "For each period, and RCP scenarios we calculated the average across all 14 models, producing a single representative dataset for each timeframe, including both the historical and RCP scenarios. The final outputs of the analysis include both individual model anomalies and ensemble-averaged datasets for each RCP scenario and time period. These datasets provide a comprehensive view of potential future hazards under different climate scenarios, offering insights into changes in extreme temperature days relative to the historical baseline. The averaged datasets were used to visualize the spatial distribution and magnitude of these hazards, as demonstrated in the notebook 04_cordex_tempDaysAbove_plots.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f68e1c82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directory containing the anomaly files\n",
    "anomalies_dir = \"/climax/indicators/cordex/tempdays/tempDaysAbove/anomalies\"\n",
    "output_dir = \"/climax/indicators/cordex/tempdays/tempDaysAbove/averaged_ensembles\"\n",
    "\n",
    "# Ensure the output directory exists\n",
    "os.makedirs(output_dir, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d29bb4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to parse filenames and extract key components\n",
    "def parse_filename(filename):\n",
    "    pattern = r\"tasmax_EUR-11_([A-Za-z0-9\\-]+)_(rcp26|rcp45|rcp85)_diff_([A-Za-z0-9\\-]+)_(above\\d+)_([\\d\\-]+)\\.nc\"\n",
    "    match = re.match(pattern, filename)\n",
    "    if match:\n",
    "        model = match.group(1)\n",
    "        scenario = match.group(2)\n",
    "        rcm = match.group(3)\n",
    "        threshold = match.group(4)  # This is now 'threshold' instead of 'percentile'\n",
    "        time_period = match.group(5)\n",
    "        return model, scenario, rcm, threshold, time_period\n",
    "    return None\n",
    "\n",
    "# Group files by scenario, percentile, and time period\n",
    "files = [f for f in os.listdir(anomalies_dir) if f.endswith(\".nc\")]\n",
    "grouped_files = {}\n",
    "\n",
    "for f in files:\n",
    "    parsed = parse_filename(f)\n",
    "    if parsed:\n",
    "        _, scenario, _, threshold, time_period = parsed\n",
    "        key = (scenario, threshold, time_period)\n",
    "        if key not in grouped_files:\n",
    "            grouped_files[key] = []\n",
    "        grouped_files[key].append(os.path.join(anomalies_dir, f))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "100bcb9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to average files and save the ensemble\n",
    "def average_ensemble(files, output_file):\n",
    "    print(files)\n",
    "    print(\"-------------------------\")\n",
    "    datasets = [xr.open_dataset(f)['tasmax'].drop_vars('height', errors='ignore') for f in files]\n",
    "    ensemble_mean = xr.concat(datasets, dim='model').mean(dim='model')\n",
    "\n",
    "    # Assign coordinates from the first dataset\n",
    "    first_ds = xr.open_dataset(files[0])\n",
    "    ensemble_mean = ensemble_mean.assign_coords({'lon': first_ds['lon'], 'lat': first_ds['lat']})\n",
    "\n",
    "    # Save the averaged dataset\n",
    "    ensemble_mean_ds = ensemble_mean.to_dataset(name='tasmax')\n",
    "    ensemble_mean_ds.to_netcdf(output_file)\n",
    "    print(f\"Averaged ensemble saved: {output_file}\")\n",
    "\n",
    "# Process each group\n",
    "for key, file_list in grouped_files.items():\n",
    "    scenario, threshold, time_period = key\n",
    "    output_filename = f\"tasmax_EUR-11_{scenario}_ensemble_temp{threshold}_{time_period}.nc\"\n",
    "    output_path = os.path.join(output_dir, output_filename)\n",
    "    average_ensemble(file_list, output_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58a12200-0053-4e52-945e-844b6ad72543",
   "metadata": {},
   "source": [
    "## Contributors\n",
    "- Giuseppe Giugliano (giuseppe.giugliano@cmcc.it)\n",
    "- Carmela de Vivo (carmela.devivo@cmcc.it)\n",
    "- Daniela Quintero (daniela.quintero@cmcc.it)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
